{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE9ddVTx6klgPCHefh8cNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktxdev/AIM-5001/blob/main/Copy_of_ConnectX_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7aR3IMyjtkS"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy kaggle-environments\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict, deque\n",
        "from kaggle_environments import evaluate, make, utils\n",
        "import random\n",
        "import math\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "O7XlDcgSLaoQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectXNet(nn.Module):\n",
        "    def __init__(self, rows=6, cols=7, action_size=7):\n",
        "        super(ConnectXNet, self).__init__()\n",
        "        self.rows, self.cols = rows, cols\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * rows * cols, 128)\n",
        "\n",
        "        # Policy head (probabilities over actions)\n",
        "        self.fc_policy = nn.Linear(128, action_size)\n",
        "\n",
        "        # Value head (expected game outcome)\n",
        "        self.fc_value = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, self.rows, self.cols)  # Shape: [batch, 1, 6, 7]\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(-1, 64 * self.rows * self.cols)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        policy = self.fc_policy(x)  # Raw logits\n",
        "        value = torch.tanh(self.fc_value(x))  # Scaled to [-1, 1]\n",
        "\n",
        "        return policy, value\n",
        "\n",
        "    def predict(self, board):\n",
        "        # Convert board to tensor and add batch dimension\n",
        "        board_tensor = torch.FloatTensor(board).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            policy_logits, value = self.forward(board_tensor)\n",
        "\n",
        "        # Mask invalid moves\n",
        "        valid_moves = [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "        policy = torch.softmax(policy_logits, dim=1).squeeze().numpy()\n",
        "        policy = np.zeros_like(policy)\n",
        "        policy[valid_moves] = 1.0 / len(valid_moves)  # Uniform policy for valid moves\n",
        "\n",
        "        return policy, value.item()"
      ],
      "metadata": {
        "id": "o1UirdAoLie4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, prior=0, parent=None):\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.total_value = 0.0\n",
        "        self.prior = prior  # From NN policy\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, c_puct=1.0):\n",
        "        self.model = model\n",
        "        self.c_puct = c_puct\n",
        "        self.nodes = {}\n",
        "\n",
        "    def is_terminal(self, board):\n",
        "        # Check if the game is over (win/draw)\n",
        "        return self.check_winner(board) != 0 or np.all(board != 0)\n",
        "\n",
        "    def get_valid_moves(self, board):\n",
        "        # Return columns where the top row is empty\n",
        "        return [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "\n",
        "    def make_move(self, board, action, player=1):\n",
        "        # Simulate a move in the column\n",
        "        new_board = board.copy()\n",
        "        for row in reversed(range(board.shape[0])):\n",
        "            if new_board[row][action] == 0:\n",
        "                new_board[row][action] = player\n",
        "                break\n",
        "        return new_board\n",
        "\n",
        "    def check_winner(self, board, inarow=4):\n",
        "        # Check horizontal, vertical, and diagonal wins\n",
        "        rows, cols = board.shape\n",
        "        for r in range(rows):\n",
        "            for c in range(cols - inarow + 1):\n",
        "                if np.all(board[r, c:c+inarow] == 1):\n",
        "                    return 1\n",
        "                elif np.all(board[r, c:c+inarow] == 2):\n",
        "                    return 2\n",
        "\n",
        "        for c in range(cols):\n",
        "            for r in range(rows - inarow + 1):\n",
        "                if np.all(board[r:r+inarow, c] == 1):\n",
        "                    return 1\n",
        "                elif np.all(board[r:r+inarow, c] == 2):\n",
        "                    return 2\n",
        "\n",
        "        for r in range(rows - inarow + 1):\n",
        "            for c in range(cols - inarow + 1):\n",
        "                if np.all(np.diag(board[r:r+inarow, c:c+inarow]) == 1):\n",
        "                    return 1\n",
        "                elif np.all(np.diag(board[r:r+inarow, c:c+inarow]) == 2):\n",
        "                    return 2\n",
        "\n",
        "                if np.all(np.diag(np.fliplr(board[r:r+inarow, c:c+inarow])) == 1):\n",
        "                    return 1\n",
        "                elif np.all(np.diag(np.fliplr(board[r:r+inarow, c:c+inarow])) == 2):\n",
        "                    return 2\n",
        "        return 0  # No winner\n",
        "\n",
        "    def search(self, board, num_simulations=50):\n",
        "        root = self.nodes.get(board.tobytes(), Node())\n",
        "\n",
        "        for _ in range(num_simulations):\n",
        "            node = root\n",
        "            sim_board = board.copy()\n",
        "\n",
        "            # Selection\n",
        "            while node.children:\n",
        "                action, node = self.select_child(node)\n",
        "                sim_board = self.make_move(sim_board, action)\n",
        "\n",
        "            # Expansion\n",
        "            if not self.is_terminal(sim_board):\n",
        "                policy, value = self.model.predict(sim_board)\n",
        "                valid_moves = self.get_valid_moves(sim_board)\n",
        "                for action in valid_moves:\n",
        "                    if action not in node.children:\n",
        "                        node.children[action] = Node(prior=policy[action], parent=node)\n",
        "\n",
        "            # Backpropagation\n",
        "            winner = self.check_winner(sim_board)\n",
        "            value = 1 if winner == 1 else -1 if winner == 2 else 0\n",
        "            while node:\n",
        "                node.visits += 1\n",
        "                node.total_value += value\n",
        "                node = node.parent\n",
        "\n",
        "        # Choose best action\n",
        "        if not root.children:\n",
        "            return random.choice(self.get_valid_moves(board))\n",
        "        best_action = max(root.children.items(), key=lambda x: x[1].visits)[0]\n",
        "        return best_action\n",
        "\n",
        "    def select_child(self, node):\n",
        "        total_visits = sum(child.visits for child in node.children.values())\n",
        "        ucb_scores = {\n",
        "            action: (child.total_value / (child.visits + 1e-6)) +\n",
        "            self.c_puct * child.prior * math.sqrt(total_visits) / (child.visits + 1)\n",
        "            for action, child in node.children.items()\n",
        "        }\n",
        "        best_action = max(ucb_scores.keys(), key=lambda a: ucb_scores[a])\n",
        "        return best_action, node.children[best_action]"
      ],
      "metadata": {
        "id": "5e3CaLEDMBGQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_play(model, num_games=100, num_simulations=50):\n",
        "    training_data = []\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        board = np.zeros((6, 7), dtype=int)\n",
        "        game_history = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            mcts = MCTS(model)\n",
        "            action = mcts.search(board, num_simulations)\n",
        "            game_history.append((board.copy(), action))\n",
        "            board = mcts.make_move(board, action)\n",
        "\n",
        "            winner = mcts.check_winner(board)\n",
        "            if winner != 0 or np.all(board != 0):\n",
        "                done = True\n",
        "\n",
        "        # Assign rewards\n",
        "        for i, (state, action) in enumerate(game_history):\n",
        "            reward = 1 if winner == 1 else -1 if winner == 2 else 0\n",
        "            training_data.append((state, action, reward))\n",
        "\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "ub9xkEIiMEWJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_games=100, num_simulations=50, epochs=10, batch_size=32):\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Generate new training data through self-play\n",
        "        training_data = []\n",
        "        for _ in range(num_games):\n",
        "            board = np.zeros((6, 7), dtype=int)\n",
        "            game_history = []\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                mcts = MCTS(model)\n",
        "                action = mcts.search(board, num_simulations)\n",
        "                game_history.append((board.copy(), action))\n",
        "                board = mcts.make_move(board, action, player=1)\n",
        "\n",
        "                # Opponent's move (random for simplicity)\n",
        "                valid_moves = mcts.get_valid_moves(board)\n",
        "                if valid_moves:\n",
        "                    opp_action = random.choice(valid_moves)\n",
        "                    board = mcts.make_move(board, opp_action, player=2)\n",
        "\n",
        "                winner = mcts.check_winner(board)\n",
        "                if winner != 0 or len(valid_moves) == 0:\n",
        "                    done = True\n",
        "\n",
        "            # Assign rewards\n",
        "            for i, (state, action) in enumerate(game_history):\n",
        "                reward = 1 if winner == 1 else -1 if winner == 2 else 0\n",
        "                training_data.append((state, action, reward))\n",
        "\n",
        "        # Train on collected data\n",
        "        random.shuffle(training_data)\n",
        "        for i in range(0, len(training_data), batch_size):\n",
        "            batch = training_data[i:i+batch_size]\n",
        "            states, actions, rewards = zip(*batch)\n",
        "\n",
        "            states = torch.FloatTensor(np.array(states))\n",
        "            actions = torch.LongTensor(actions)\n",
        "            rewards = torch.FloatTensor(rewards)\n",
        "\n",
        "            # Forward pass\n",
        "            policy_logits, values = model(states)\n",
        "\n",
        "            # Loss\n",
        "            policy_loss = nn.CrossEntropyLoss()(policy_logits, actions)\n",
        "            value_loss = nn.MSELoss()(values.squeeze(), rewards)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "eBTiOs7NK0x8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConnectXNet()\n",
        "# training_data = self_play(model, num_games=100)\n",
        "train(model, num_games=100, epochs=10)\n",
        "torch.save(model.state_dict(), \"alphazero_connectx.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8EuEcTPK5Ka",
        "outputId": "b31fe7f0-9c74-45af-ea89-8b8e6db86948"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.6029398441314697\n",
            "Epoch 1, Loss: 0.3377048969268799\n",
            "Epoch 2, Loss: 1.4077250957489014\n",
            "Epoch 3, Loss: 0.8541877269744873\n",
            "Epoch 4, Loss: 1.4834400415420532\n",
            "Epoch 5, Loss: 0.29875826835632324\n",
            "Epoch 6, Loss: 0.992655873298645\n",
            "Epoch 7, Loss: 0.9899258017539978\n",
            "Epoch 8, Loss: 0.637650728225708\n",
            "Epoch 9, Loss: 0.7171280980110168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_state_dict = torch.load(\"alphazero_connectx.pth\")\n",
        "model_state_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0DKN7rISAra",
        "outputId": "55195398-b3c9-4c64-f46e-8b3dc441c725"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('conv1.weight',\n",
              "              tensor([[[[ 0.0842, -0.1090, -0.0640],\n",
              "                        [ 0.2493, -0.1721,  0.2155],\n",
              "                        [-0.2594, -0.2080,  0.1645]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0157,  0.2929, -0.2065],\n",
              "                        [-0.2107, -0.2899,  0.3240],\n",
              "                        [-0.1476, -0.3428,  0.1855]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2952,  0.1080,  0.0553],\n",
              "                        [-0.0260,  0.1976,  0.0232],\n",
              "                        [ 0.0683, -0.0431,  0.2905]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3628, -0.1380, -0.1927],\n",
              "                        [-0.0091, -0.3586, -0.2887],\n",
              "                        [-0.2348, -0.0058, -0.0912]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0149, -0.0370,  0.3532],\n",
              "                        [-0.0198, -0.1872,  0.2462],\n",
              "                        [-0.1544,  0.2512,  0.2096]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0094, -0.2249, -0.0803],\n",
              "                        [-0.1152,  0.2526,  0.2112],\n",
              "                        [-0.2982,  0.2966,  0.3080]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1740, -0.1679, -0.2632],\n",
              "                        [-0.2719,  0.0460, -0.0928],\n",
              "                        [-0.0995, -0.2096,  0.2922]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1035, -0.0409,  0.3202],\n",
              "                        [ 0.0970,  0.2313,  0.1962],\n",
              "                        [-0.1483,  0.2699, -0.0794]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0700,  0.2635,  0.1690],\n",
              "                        [ 0.1875,  0.2678,  0.0999],\n",
              "                        [ 0.1119,  0.1146,  0.2176]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1104, -0.1932,  0.2107],\n",
              "                        [ 0.0552, -0.0049,  0.2941],\n",
              "                        [-0.1017, -0.0368,  0.1650]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0928,  0.0480,  0.1245],\n",
              "                        [-0.0648, -0.0975,  0.1596],\n",
              "                        [-0.3427, -0.0845,  0.1054]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2027,  0.0295,  0.2892],\n",
              "                        [ 0.2996, -0.0265,  0.0541],\n",
              "                        [ 0.0129, -0.1012, -0.2718]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0943, -0.1182,  0.1965],\n",
              "                        [ 0.2093, -0.2750,  0.2486],\n",
              "                        [-0.1818, -0.0118,  0.1596]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1384,  0.0722,  0.1636],\n",
              "                        [-0.1402,  0.1762, -0.2229],\n",
              "                        [-0.1859, -0.0507,  0.2817]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3313,  0.2920, -0.1923],\n",
              "                        [-0.0309, -0.1977, -0.2041],\n",
              "                        [-0.1073, -0.0534,  0.0875]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2645, -0.2123, -0.1222],\n",
              "                        [-0.1456,  0.0578, -0.1293],\n",
              "                        [ 0.1351, -0.3469, -0.2572]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1999, -0.3362,  0.1494],\n",
              "                        [ 0.0148, -0.0976, -0.0711],\n",
              "                        [-0.2778, -0.3147, -0.0876]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3197, -0.0072, -0.0438],\n",
              "                        [ 0.3019,  0.1000,  0.3092],\n",
              "                        [ 0.3237,  0.0130,  0.0020]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1359, -0.2142,  0.0616],\n",
              "                        [ 0.1113, -0.0305, -0.1697],\n",
              "                        [-0.2727,  0.3617, -0.2493]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2441, -0.2081,  0.2844],\n",
              "                        [ 0.3493,  0.0815, -0.0135],\n",
              "                        [ 0.0623, -0.3381, -0.1916]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2124, -0.2755,  0.0032],\n",
              "                        [ 0.1423,  0.1272, -0.1154],\n",
              "                        [-0.1015,  0.0014,  0.0142]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0973,  0.3194,  0.0341],\n",
              "                        [-0.0660,  0.2573,  0.2652],\n",
              "                        [-0.1798, -0.2448, -0.0472]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0899, -0.3344,  0.0271],\n",
              "                        [-0.1130,  0.1519, -0.0715],\n",
              "                        [-0.1555,  0.0305,  0.1896]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0692,  0.1931, -0.2567],\n",
              "                        [-0.2596, -0.0681,  0.3067],\n",
              "                        [ 0.1712, -0.3190, -0.2917]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3160,  0.1955, -0.1371],\n",
              "                        [ 0.2532, -0.0140, -0.1304],\n",
              "                        [ 0.0520,  0.1704,  0.2485]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2435, -0.0225,  0.0102],\n",
              "                        [ 0.0121,  0.1652, -0.2621],\n",
              "                        [-0.2628,  0.2560, -0.1798]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2138, -0.0810, -0.0910],\n",
              "                        [ 0.2855,  0.3356, -0.2514],\n",
              "                        [ 0.0839,  0.1847,  0.1146]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3024, -0.3263,  0.1043],\n",
              "                        [ 0.2213,  0.2112, -0.1461],\n",
              "                        [-0.1792, -0.1569,  0.0304]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1360,  0.1952, -0.1785],\n",
              "                        [-0.1595, -0.0379,  0.1812],\n",
              "                        [-0.1719, -0.3231, -0.2056]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2590, -0.2038,  0.1394],\n",
              "                        [-0.3468,  0.2985, -0.0231],\n",
              "                        [-0.0274,  0.1829,  0.2376]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0692,  0.0713,  0.1202],\n",
              "                        [-0.0094, -0.2738, -0.0076],\n",
              "                        [ 0.3033,  0.0993,  0.3180]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2539,  0.1014, -0.2505],\n",
              "                        [ 0.1262,  0.0656,  0.0662],\n",
              "                        [-0.0345,  0.1449, -0.0167]]]])),\n",
              "             ('conv1.bias',\n",
              "              tensor([-0.1517, -0.0750,  0.0433,  0.0559, -0.2697, -0.2346, -0.3053, -0.2099,\n",
              "                      -0.0649, -0.2433,  0.2399, -0.1136,  0.0711,  0.0192,  0.2545,  0.1532,\n",
              "                       0.2365, -0.1575, -0.0067, -0.0058,  0.3118,  0.3301,  0.0600,  0.2894,\n",
              "                       0.2045, -0.0528, -0.0888, -0.0452, -0.2847,  0.1628,  0.1018, -0.1739])),\n",
              "             ('conv2.weight',\n",
              "              tensor([[[[ 0.0663,  0.0258, -0.0467],\n",
              "                        [ 0.0154,  0.0370, -0.0072],\n",
              "                        [-0.0467, -0.0348,  0.0229]],\n",
              "              \n",
              "                       [[-0.0376,  0.0099,  0.0434],\n",
              "                        [-0.0232, -0.0203,  0.0112],\n",
              "                        [ 0.0402,  0.0683, -0.0011]],\n",
              "              \n",
              "                       [[ 0.0248, -0.0044, -0.0371],\n",
              "                        [ 0.0591,  0.0180, -0.0329],\n",
              "                        [-0.0050,  0.0555,  0.0032]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-0.0613,  0.0590,  0.0022],\n",
              "                        [ 0.0575, -0.0348, -0.0074],\n",
              "                        [-0.0004,  0.0357,  0.0102]],\n",
              "              \n",
              "                       [[ 0.0395, -0.0026, -0.0457],\n",
              "                        [ 0.0291,  0.0218,  0.0110],\n",
              "                        [-0.0302,  0.0694, -0.0197]],\n",
              "              \n",
              "                       [[ 0.0690, -0.0117, -0.0293],\n",
              "                        [ 0.0127,  0.0002,  0.0519],\n",
              "                        [ 0.0358,  0.0397,  0.0133]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0255,  0.0416,  0.0568],\n",
              "                        [-0.0380, -0.0328, -0.0392],\n",
              "                        [ 0.0228, -0.0008,  0.0272]],\n",
              "              \n",
              "                       [[-0.0262,  0.0074, -0.0242],\n",
              "                        [-0.0450, -0.0519, -0.0453],\n",
              "                        [ 0.0065, -0.0368,  0.0338]],\n",
              "              \n",
              "                       [[-0.0159, -0.0554, -0.0222],\n",
              "                        [ 0.0093, -0.0308,  0.0302],\n",
              "                        [-0.0015, -0.0407,  0.0613]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 0.0408, -0.0012,  0.0328],\n",
              "                        [-0.0138,  0.0310, -0.0164],\n",
              "                        [-0.0372,  0.0675,  0.0082]],\n",
              "              \n",
              "                       [[ 0.0346,  0.0083, -0.0059],\n",
              "                        [ 0.0216, -0.0370, -0.0311],\n",
              "                        [ 0.0341, -0.0107,  0.0352]],\n",
              "              \n",
              "                       [[-0.0122, -0.0377,  0.0204],\n",
              "                        [ 0.0755, -0.0070,  0.0139],\n",
              "                        [ 0.0712, -0.0437,  0.0130]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0245,  0.0024,  0.0356],\n",
              "                        [-0.0132,  0.0346, -0.0090],\n",
              "                        [ 0.0146, -0.0536,  0.0403]],\n",
              "              \n",
              "                       [[ 0.0217, -0.0599,  0.0112],\n",
              "                        [-0.0009, -0.0283, -0.0324],\n",
              "                        [-0.0194, -0.0529, -0.0390]],\n",
              "              \n",
              "                       [[ 0.0130,  0.0113,  0.0506],\n",
              "                        [-0.0365, -0.0204, -0.0335],\n",
              "                        [ 0.0222, -0.0167, -0.0316]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-0.0087,  0.0255,  0.0249],\n",
              "                        [ 0.0300,  0.0511,  0.0539],\n",
              "                        [-0.0043,  0.0483,  0.0234]],\n",
              "              \n",
              "                       [[ 0.0208,  0.0251,  0.0097],\n",
              "                        [ 0.0057, -0.0585,  0.0312],\n",
              "                        [-0.0274, -0.0016, -0.0549]],\n",
              "              \n",
              "                       [[ 0.0290, -0.0351, -0.0691],\n",
              "                        [-0.0052,  0.0523, -0.0163],\n",
              "                        [ 0.0486, -0.0363,  0.0186]]],\n",
              "              \n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "              \n",
              "                      [[[-0.0045,  0.0391,  0.0184],\n",
              "                        [-0.0126, -0.0252,  0.0480],\n",
              "                        [ 0.0225, -0.0064, -0.0313]],\n",
              "              \n",
              "                       [[ 0.0168,  0.0195,  0.0033],\n",
              "                        [ 0.0325,  0.0245,  0.0475],\n",
              "                        [ 0.0395,  0.0463, -0.0315]],\n",
              "              \n",
              "                       [[ 0.0397, -0.0443,  0.0435],\n",
              "                        [-0.0412, -0.0293, -0.0425],\n",
              "                        [-0.0008, -0.0482,  0.0213]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 0.0095,  0.0385, -0.0110],\n",
              "                        [ 0.0365, -0.0212,  0.0261],\n",
              "                        [ 0.0172,  0.0346,  0.0118]],\n",
              "              \n",
              "                       [[-0.0581, -0.0429,  0.0392],\n",
              "                        [ 0.0005, -0.0435, -0.0358],\n",
              "                        [ 0.0279,  0.0288,  0.0283]],\n",
              "              \n",
              "                       [[-0.0387, -0.0449,  0.0295],\n",
              "                        [-0.0395, -0.0482,  0.0063],\n",
              "                        [ 0.0011, -0.0330, -0.0358]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0171,  0.0729, -0.0303],\n",
              "                        [-0.0577,  0.0332, -0.0046],\n",
              "                        [ 0.0186, -0.0275,  0.0415]],\n",
              "              \n",
              "                       [[ 0.0174,  0.0416,  0.0344],\n",
              "                        [ 0.0072,  0.0464, -0.0307],\n",
              "                        [-0.0391, -0.0243, -0.0210]],\n",
              "              \n",
              "                       [[ 0.0510, -0.0460,  0.0420],\n",
              "                        [ 0.0213, -0.0069,  0.0559],\n",
              "                        [-0.0089, -0.0390, -0.0090]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-0.0370, -0.0085,  0.0070],\n",
              "                        [-0.0449,  0.0085, -0.0135],\n",
              "                        [-0.0459, -0.0113, -0.0562]],\n",
              "              \n",
              "                       [[-0.0441, -0.0089,  0.0491],\n",
              "                        [-0.0213,  0.0361,  0.0001],\n",
              "                        [ 0.0364, -0.0137, -0.0130]],\n",
              "              \n",
              "                       [[ 0.0250, -0.0226, -0.0604],\n",
              "                        [ 0.0206,  0.0193,  0.0101],\n",
              "                        [ 0.0221,  0.0295, -0.0056]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0554,  0.0264,  0.0211],\n",
              "                        [ 0.0278,  0.0269, -0.0281],\n",
              "                        [ 0.0020, -0.0280, -0.0185]],\n",
              "              \n",
              "                       [[-0.0606, -0.0028,  0.0379],\n",
              "                        [ 0.0199, -0.0457,  0.0465],\n",
              "                        [ 0.0545, -0.0606,  0.0280]],\n",
              "              \n",
              "                       [[-0.0055, -0.0203,  0.0094],\n",
              "                        [-0.0398, -0.0598, -0.0267],\n",
              "                        [-0.0026, -0.0471, -0.0577]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 0.0239,  0.0073, -0.0191],\n",
              "                        [ 0.0243, -0.0603, -0.0014],\n",
              "                        [-0.0251, -0.0059,  0.0296]],\n",
              "              \n",
              "                       [[-0.0165,  0.0346,  0.0500],\n",
              "                        [ 0.0394, -0.0348, -0.0070],\n",
              "                        [ 0.0356,  0.0360, -0.0236]],\n",
              "              \n",
              "                       [[ 0.0734,  0.0309, -0.0280],\n",
              "                        [-0.0167,  0.0267, -0.0442],\n",
              "                        [ 0.0518, -0.0425, -0.0167]]]])),\n",
              "             ('conv2.bias',\n",
              "              tensor([-0.0433,  0.0411, -0.0208,  0.0540,  0.0207, -0.0145, -0.0311,  0.0175,\n",
              "                       0.0022, -0.0255, -0.0265,  0.0396,  0.0677,  0.0153, -0.0003, -0.0383,\n",
              "                       0.0213, -0.0494,  0.0011,  0.0649,  0.0390,  0.0533, -0.0358,  0.0138,\n",
              "                      -0.0193,  0.0011,  0.0092, -0.0279,  0.0551, -0.0322,  0.0654,  0.0192,\n",
              "                      -0.0010,  0.0561, -0.0518, -0.0025, -0.0190, -0.0460,  0.0556,  0.0573,\n",
              "                       0.0033,  0.0257,  0.0663, -0.0251, -0.0006,  0.0231, -0.0513, -0.0586,\n",
              "                       0.0515,  0.0312, -0.0199,  0.0435,  0.0545,  0.0513,  0.0086,  0.0498,\n",
              "                      -0.0375,  0.0187,  0.0553,  0.0280, -0.0321,  0.0707,  0.0313, -0.0253])),\n",
              "             ('fc1.weight',\n",
              "              tensor([[-2.6583e-04, -1.4104e-03, -4.8600e-03,  ...,  1.5371e-02,\n",
              "                       -1.3208e-02, -6.0960e-03],\n",
              "                      [ 1.1155e-02, -1.7011e-02, -1.8255e-02,  ...,  1.3360e-04,\n",
              "                        4.0665e-03,  2.4916e-05],\n",
              "                      [-2.5783e-02, -3.3977e-02,  2.5496e-02,  ...,  1.2774e-02,\n",
              "                        1.4272e-02, -3.2150e-03],\n",
              "                      ...,\n",
              "                      [-5.9672e-03,  1.3133e-02, -1.5923e-02,  ..., -4.1248e-04,\n",
              "                        3.6043e-03, -6.3748e-04],\n",
              "                      [ 7.1866e-03, -1.4078e-02,  1.5904e-02,  ...,  1.5073e-03,\n",
              "                       -1.1652e-02, -3.6091e-03],\n",
              "                      [-1.6473e-02, -1.7769e-02, -1.6027e-02,  ..., -1.3129e-02,\n",
              "                        1.5152e-02, -2.1523e-02]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([-0.0055, -0.0195,  0.0222, -0.0148,  0.0148,  0.0202,  0.0034,  0.0009,\n",
              "                      -0.0051, -0.0248, -0.0014,  0.0111, -0.0191,  0.0269, -0.0010, -0.0090,\n",
              "                       0.0016,  0.0074, -0.0172,  0.0017, -0.0056, -0.0165,  0.0323,  0.0090,\n",
              "                       0.0170, -0.0156,  0.0044, -0.0154, -0.0067, -0.0116,  0.0053, -0.0190,\n",
              "                      -0.0045,  0.0069,  0.0180, -0.0169, -0.0082, -0.0152,  0.0171, -0.0133,\n",
              "                       0.0028, -0.0150,  0.0260, -0.0003,  0.0090,  0.0314,  0.0035,  0.0119,\n",
              "                      -0.0221,  0.0161,  0.0103, -0.0228, -0.0239,  0.0071,  0.0127,  0.0073,\n",
              "                      -0.0141,  0.0116,  0.0039,  0.0093,  0.0001, -0.0186,  0.0097,  0.0151,\n",
              "                      -0.0061,  0.0252, -0.0148,  0.0123, -0.0159,  0.0221,  0.0019,  0.0154,\n",
              "                       0.0089,  0.0020,  0.0218,  0.0045, -0.0013,  0.0106,  0.0061, -0.0155,\n",
              "                       0.0160, -0.0200, -0.0046, -0.0247,  0.0323,  0.0096,  0.0294,  0.0305,\n",
              "                       0.0123, -0.0006, -0.0131, -0.0211,  0.0285,  0.0030, -0.0082,  0.0241,\n",
              "                      -0.0029, -0.0032,  0.0045,  0.0132, -0.0095,  0.0031,  0.0108,  0.0136,\n",
              "                      -0.0198,  0.0019,  0.0104,  0.0010, -0.0129, -0.0149, -0.0192,  0.0178,\n",
              "                      -0.0224,  0.0046, -0.0210, -0.0108,  0.0155,  0.0016,  0.0232, -0.0160,\n",
              "                      -0.0050, -0.0141, -0.0153,  0.0057, -0.0188,  0.0035,  0.0118, -0.0039])),\n",
              "             ('fc_policy.weight',\n",
              "              tensor([[ 3.7648e-02, -5.0087e-02,  7.0998e-02, -7.7840e-02,  6.5441e-02,\n",
              "                        2.1994e-02,  6.4364e-02,  3.6920e-02,  1.3207e-02,  1.4283e-02,\n",
              "                       -7.4121e-02, -3.7630e-02, -6.1190e-02,  1.1592e-01, -7.3407e-02,\n",
              "                        5.4150e-02, -5.4293e-02,  4.4981e-02, -3.7602e-02, -3.5150e-03,\n",
              "                       -7.3122e-02,  1.9596e-02,  6.8814e-02,  1.4870e-01,  8.9480e-02,\n",
              "                        7.8705e-02, -3.6609e-02,  5.1319e-03,  5.7812e-02, -1.0423e-02,\n",
              "                       -7.7870e-03, -5.5605e-02, -6.8110e-02, -4.7651e-02,  3.3884e-02,\n",
              "                        1.5075e-02, -1.4446e-02, -1.0170e-01, -1.2109e-02, -1.9065e-02,\n",
              "                       -6.5542e-02,  2.3591e-02,  7.7955e-02,  6.5129e-02,  1.1272e-01,\n",
              "                        9.5650e-02,  5.2470e-02,  5.8533e-02, -5.8058e-02,  2.7013e-02,\n",
              "                       -7.7406e-02,  4.8033e-02, -7.1580e-02,  5.5421e-02,  1.2350e-01,\n",
              "                       -5.8295e-02,  7.8624e-02,  8.8502e-02, -3.1781e-02,  1.1892e-01,\n",
              "                        9.8930e-02,  3.2802e-02, -1.7407e-01,  8.0947e-02,  7.8638e-03,\n",
              "                        7.3008e-02,  9.3783e-03, -8.5966e-03, -8.1131e-02,  1.7359e-02,\n",
              "                        8.3534e-03,  1.1731e-01, -2.9230e-02, -3.8455e-02,  3.3733e-02,\n",
              "                        1.0034e-01, -1.0494e-01,  3.5584e-02, -8.6106e-02,  4.7947e-02,\n",
              "                        7.1044e-02,  1.0260e-02,  1.3514e-03, -6.9671e-03,  1.0620e-01,\n",
              "                        1.2622e-02,  4.9202e-02,  7.9988e-02, -6.8087e-02,  7.6943e-02,\n",
              "                        6.3319e-02, -5.8253e-02,  1.0801e-01,  3.9855e-02, -7.8338e-02,\n",
              "                        3.9928e-02,  6.8313e-02,  6.5173e-02,  8.4389e-02,  4.4630e-02,\n",
              "                        1.1478e-02,  4.4752e-02, -4.1432e-02,  6.8922e-02, -1.9395e-02,\n",
              "                        8.1410e-02, -5.9261e-02,  1.0118e-02, -3.4603e-02, -6.1137e-02,\n",
              "                       -5.7684e-02,  3.5702e-02, -7.8787e-02, -6.7980e-02, -3.2843e-02,\n",
              "                       -3.0556e-02,  7.9053e-02, -4.2693e-03,  4.6631e-02, -9.8639e-02,\n",
              "                       -3.5949e-02, -7.2140e-02, -2.8992e-02,  8.9087e-02,  6.0782e-02,\n",
              "                       -6.0671e-02,  7.3154e-03,  1.2735e-02],\n",
              "                      [-3.3758e-02, -7.5830e-02, -3.7331e-02,  2.6239e-02,  6.2758e-02,\n",
              "                       -6.6287e-02,  5.5891e-02,  3.0968e-02,  8.1924e-02, -6.8860e-02,\n",
              "                        3.7960e-02,  9.5548e-02,  1.6494e-03, -8.7115e-02,  9.3456e-02,\n",
              "                        1.1306e-04, -6.9417e-02,  5.1874e-02,  7.0934e-02,  2.8837e-02,\n",
              "                        4.9549e-02, -2.5430e-02,  7.8961e-02, -5.8846e-02, -8.9649e-02,\n",
              "                        9.2798e-02, -4.4617e-02, -4.2408e-02,  8.7065e-02, -8.8266e-02,\n",
              "                        5.5358e-02, -5.5649e-02, -7.3200e-02,  4.6803e-02, -7.9567e-02,\n",
              "                        3.6933e-02,  1.5716e-02,  8.4578e-02,  3.2410e-02, -7.0703e-03,\n",
              "                        4.2830e-02,  4.3120e-02,  3.4071e-02,  3.4226e-02, -1.8339e-02,\n",
              "                        8.1242e-02,  2.9287e-02, -6.4113e-02, -7.2031e-02,  3.6464e-02,\n",
              "                        1.4974e-02,  6.1388e-02, -2.8504e-03, -1.6076e-02, -4.8899e-02,\n",
              "                        1.8753e-02, -2.2889e-02,  7.2404e-02,  4.5662e-02, -8.0197e-03,\n",
              "                       -9.4841e-02,  1.8805e-02,  9.2985e-03,  8.2933e-03, -2.5745e-02,\n",
              "                        8.5833e-02,  8.9459e-02, -2.5913e-02,  3.5203e-02,  5.9308e-02,\n",
              "                       -2.9932e-02, -3.1666e-02,  2.6897e-02,  7.3789e-02,  6.9235e-02,\n",
              "                       -7.2028e-02,  9.0081e-02,  9.8338e-02,  6.8613e-02,  4.3238e-02,\n",
              "                        9.2239e-02,  3.4184e-02,  8.7923e-02, -6.9620e-02,  5.6149e-02,\n",
              "                       -8.7126e-03, -2.3563e-02,  4.8807e-02, -8.0284e-02, -5.2036e-02,\n",
              "                        4.5388e-02, -2.3331e-02, -3.6104e-02, -8.7134e-02, -3.2688e-02,\n",
              "                        6.4189e-02, -3.9285e-02,  8.8475e-03, -6.9258e-03,  9.6291e-02,\n",
              "                       -8.7192e-02, -4.8325e-02, -5.5173e-02,  2.3356e-02,  4.3845e-02,\n",
              "                       -1.1404e-01,  9.0153e-03,  3.8569e-02, -2.7990e-02, -9.3191e-02,\n",
              "                        8.2125e-02,  1.8758e-02, -8.9521e-02, -7.7268e-02, -3.7019e-02,\n",
              "                       -4.0470e-02,  5.5149e-02,  2.8455e-02,  1.5427e-02,  1.0064e-02,\n",
              "                       -1.6165e-02, -8.9538e-04, -1.9392e-02, -4.7442e-03, -1.0209e-02,\n",
              "                        2.3822e-03, -7.0505e-02, -8.2188e-02],\n",
              "                      [-8.2352e-02,  3.0673e-02, -7.5899e-02,  2.0454e-02,  5.4871e-02,\n",
              "                        5.8203e-02, -3.4135e-02, -1.3994e-02, -7.2561e-02,  3.5868e-02,\n",
              "                       -1.6071e-02, -1.9104e-02,  6.8246e-02,  3.1266e-02, -1.7789e-02,\n",
              "                       -4.4093e-02,  5.0192e-02, -5.9422e-02, -2.1039e-03, -7.1205e-02,\n",
              "                        1.7937e-02,  5.1220e-02, -3.3781e-02, -2.1445e-03, -9.4191e-02,\n",
              "                       -4.0543e-03,  5.8027e-02, -5.3733e-02, -2.2602e-02, -5.1671e-02,\n",
              "                       -5.9084e-02, -9.3365e-02, -1.4360e-03, -9.2328e-02,  1.1066e-01,\n",
              "                       -6.4296e-02,  6.7626e-02,  1.3574e-02, -5.4289e-02,  2.2486e-02,\n",
              "                        7.8554e-02, -2.5912e-02, -9.8510e-02, -6.0490e-02, -3.2759e-02,\n",
              "                        1.7665e-02,  3.6548e-02, -2.1217e-02, -6.0271e-02,  2.5634e-02,\n",
              "                        1.5165e-02, -1.0305e-01, -4.4329e-02,  7.2478e-02, -1.2648e-01,\n",
              "                        7.7447e-02, -9.1893e-02,  4.6894e-02,  8.1059e-02,  4.8068e-02,\n",
              "                       -1.1729e-02,  3.9900e-03,  9.2853e-02, -2.6998e-02,  3.3116e-02,\n",
              "                       -2.4430e-02,  8.1360e-02,  5.0060e-02,  3.3531e-02, -6.7945e-02,\n",
              "                       -6.5004e-02,  1.6000e-02, -1.3829e-03,  4.2808e-02,  3.3276e-02,\n",
              "                       -7.3003e-02,  4.2854e-02,  2.1838e-02,  8.9612e-02,  5.5088e-02,\n",
              "                       -6.2834e-02,  4.8010e-02,  3.7475e-03, -3.5297e-02,  2.1871e-03,\n",
              "                       -6.1437e-02, -8.2590e-02, -8.0141e-02, -1.7599e-02,  2.8056e-02,\n",
              "                        2.7217e-02,  6.5589e-02, -8.7357e-02, -4.2423e-02, -5.1606e-02,\n",
              "                        3.0484e-02, -2.4030e-02,  3.4048e-02, -5.0241e-02, -9.6522e-02,\n",
              "                        6.0696e-02, -1.3608e-04, -2.7521e-02,  5.7017e-03, -3.3963e-02,\n",
              "                        2.4020e-02, -3.7185e-02, -2.8629e-02,  1.4717e-02, -6.3380e-02,\n",
              "                       -9.0137e-02, -7.5060e-02,  2.7960e-03, -6.5511e-02, -3.7332e-02,\n",
              "                        3.6033e-02,  2.7794e-02,  1.9557e-02, -7.8197e-02, -3.5371e-02,\n",
              "                        5.5568e-02, -3.3306e-02, -4.4130e-02, -1.6180e-02,  1.2556e-02,\n",
              "                        7.7611e-02,  1.7330e-02,  2.2692e-03],\n",
              "                      [-4.3429e-02, -6.7826e-02,  2.5515e-02,  2.4859e-02,  1.4871e-02,\n",
              "                       -5.5987e-02,  1.1830e-02,  1.1118e-02,  7.5958e-02, -1.5823e-02,\n",
              "                       -4.2275e-02, -3.0939e-02, -2.8068e-02,  4.4246e-02, -6.5624e-02,\n",
              "                        3.0614e-02, -3.2638e-02, -6.2625e-02,  5.4657e-02, -3.6342e-02,\n",
              "                       -2.5514e-02,  8.5604e-03, -7.9815e-02,  7.9728e-03,  1.2444e-02,\n",
              "                        3.9657e-02, -7.2652e-02, -6.4107e-03,  4.0106e-02, -8.0040e-02,\n",
              "                       -4.8360e-02, -4.9022e-02,  6.0453e-02, -9.2615e-02,  3.9875e-02,\n",
              "                       -9.1624e-02, -8.0136e-02, -1.6154e-02,  6.3470e-02, -4.5914e-02,\n",
              "                        5.9047e-02,  7.2540e-02,  6.8434e-02,  6.4257e-02, -7.0785e-02,\n",
              "                        9.4239e-02,  7.1149e-02, -4.5681e-02,  4.8830e-02, -5.6393e-02,\n",
              "                        3.3113e-02,  1.1611e-02,  5.4599e-02, -9.4438e-02, -5.1222e-02,\n",
              "                       -9.0529e-03, -7.5978e-02, -5.3837e-03,  2.8463e-02,  1.9745e-02,\n",
              "                       -5.0830e-02, -8.1710e-02,  6.3107e-02,  2.1214e-02,  3.5836e-02,\n",
              "                        6.1602e-02, -1.2509e-03,  5.1789e-02,  2.1883e-02, -5.7126e-02,\n",
              "                       -5.2651e-03, -7.1753e-02,  7.1704e-02,  4.7526e-03,  1.0711e-02,\n",
              "                       -3.3015e-02,  3.6902e-02, -3.9212e-02,  4.1590e-02,  3.9386e-02,\n",
              "                        7.6903e-02, -5.5020e-02, -3.7836e-02,  6.4914e-02,  6.6891e-02,\n",
              "                       -1.8291e-02, -7.9677e-02, -7.4924e-03, -7.2247e-02,  2.4137e-02,\n",
              "                       -5.6257e-02, -1.0285e-03, -9.1635e-02, -7.2138e-04,  4.7324e-02,\n",
              "                       -5.1090e-03,  2.2473e-02, -8.9242e-02,  9.0493e-03, -6.3889e-02,\n",
              "                       -5.0248e-02, -4.3792e-02, -5.7247e-02, -6.7573e-02, -8.8116e-02,\n",
              "                       -9.3488e-02, -6.5020e-03, -3.3243e-02,  6.4643e-02,  3.5162e-02,\n",
              "                        3.6391e-02, -1.3939e-02,  1.5623e-02,  1.9998e-02, -2.1892e-02,\n",
              "                       -4.5192e-03,  1.0559e-02,  1.6938e-02, -6.8919e-02,  4.1125e-02,\n",
              "                       -9.3647e-02, -7.3752e-02, -4.0812e-02, -3.5611e-02,  6.5696e-02,\n",
              "                        3.0578e-02, -4.5377e-02, -6.6730e-02],\n",
              "                      [-6.6142e-02,  6.0229e-02, -1.0290e-01, -5.3855e-02,  4.1718e-02,\n",
              "                       -8.7995e-02, -7.5759e-02,  3.9989e-03,  1.5034e-02,  5.2602e-03,\n",
              "                        6.9727e-02, -6.0275e-02,  4.8559e-02,  5.3118e-02, -5.6030e-04,\n",
              "                       -2.9327e-02, -7.0322e-04, -7.2866e-02,  4.8822e-02,  5.7212e-02,\n",
              "                       -8.1752e-02, -3.3862e-02, -4.8016e-02, -1.3134e-01, -1.0814e-01,\n",
              "                       -4.6077e-02,  3.6357e-02, -3.9990e-02, -1.0832e-01, -9.0156e-02,\n",
              "                        6.8398e-02,  5.8189e-02,  1.6147e-02,  7.5634e-02, -9.3709e-02,\n",
              "                       -5.4674e-02, -5.4113e-02,  4.5193e-03, -3.3072e-02,  3.3783e-02,\n",
              "                       -8.7238e-02,  4.1389e-02, -1.0613e-01,  7.3947e-02, -6.6516e-02,\n",
              "                       -3.9926e-02,  7.9697e-02,  8.0666e-02, -9.0673e-02,  1.3662e-02,\n",
              "                       -4.2075e-02, -5.4796e-02, -1.8753e-02,  3.1007e-02,  3.7816e-02,\n",
              "                       -5.3563e-02, -6.9757e-02, -2.2034e-02,  5.6802e-02,  3.7752e-02,\n",
              "                        2.1912e-02,  5.6879e-02, -7.2269e-02, -1.7996e-02,  5.5916e-02,\n",
              "                       -7.5568e-02, -2.3298e-02, -4.7932e-02,  7.0976e-02,  7.4949e-03,\n",
              "                       -2.9251e-02,  6.5597e-02, -8.2547e-02, -1.0563e-01, -3.8784e-02,\n",
              "                       -8.4328e-03, -1.0189e-01, -1.0016e-01, -5.6338e-02, -3.0286e-02,\n",
              "                       -1.1561e-01, -3.7515e-02,  6.6818e-02,  3.9317e-02,  3.9061e-02,\n",
              "                       -7.1309e-02, -1.1151e-01, -9.2272e-02,  3.9559e-02, -5.6654e-02,\n",
              "                        6.4486e-02, -8.2897e-02, -2.3432e-02, -5.3904e-03,  2.5055e-02,\n",
              "                       -6.1058e-02, -3.7641e-02,  4.7752e-02,  6.6454e-02,  1.7378e-02,\n",
              "                        4.2090e-02,  2.7604e-02, -5.0044e-02,  6.0859e-02,  4.5553e-02,\n",
              "                       -7.0894e-03, -4.2575e-02,  5.8987e-02, -8.7113e-03,  3.6977e-02,\n",
              "                       -7.3060e-02, -5.3397e-02, -5.9215e-02,  2.2853e-02, -4.6963e-02,\n",
              "                        7.2785e-02, -2.4295e-02, -5.9550e-02,  3.3638e-02, -7.6749e-03,\n",
              "                       -7.0571e-02, -7.0608e-02, -5.8929e-02, -7.4566e-02,  4.1183e-03,\n",
              "                       -9.4305e-02, -3.3137e-02,  7.9427e-02],\n",
              "                      [ 4.3613e-02,  2.5312e-02,  4.0452e-02, -3.1043e-02, -1.1277e-01,\n",
              "                       -6.0004e-02,  3.8557e-02, -8.8381e-02,  3.1082e-02, -6.6660e-02,\n",
              "                        6.4193e-02, -9.3290e-02, -4.2714e-02, -6.5965e-03,  3.4827e-02,\n",
              "                        1.4684e-02,  4.8910e-02, -3.3412e-02,  6.2798e-02, -9.0978e-02,\n",
              "                       -1.4258e-02, -8.6085e-03, -1.1747e-01, -1.4757e-01, -1.5524e-02,\n",
              "                        1.8040e-02,  4.0238e-02,  3.4308e-02, -1.1951e-02,  5.4007e-02,\n",
              "                        5.2074e-03, -7.0152e-03, -6.3732e-02, -5.7210e-02, -8.2166e-02,\n",
              "                        4.3738e-02, -2.4046e-02, -1.3605e-01,  1.6812e-03, -6.3830e-02,\n",
              "                       -1.9806e-02,  3.9261e-02,  3.9285e-02,  1.3709e-02, -4.3583e-03,\n",
              "                        3.9555e-02,  5.3698e-02, -1.3267e-02, -9.0572e-02, -5.1192e-02,\n",
              "                        2.7021e-02, -2.1754e-03,  6.4233e-02, -1.0600e-02, -6.5724e-02,\n",
              "                       -1.2324e-02,  6.4248e-02, -2.5956e-02, -6.3929e-02,  6.1212e-02,\n",
              "                       -5.1643e-02,  4.2687e-02, -7.4524e-02, -5.2916e-02,  4.8194e-02,\n",
              "                       -1.0661e-01,  6.8925e-02,  7.9764e-02, -7.2378e-02,  2.3641e-02,\n",
              "                        6.9378e-02,  5.3108e-02, -8.5211e-02, -2.6769e-03,  4.2615e-02,\n",
              "                       -6.2604e-02, -1.5533e-01, -1.0810e-01, -1.0826e-01, -6.9562e-02,\n",
              "                       -9.0003e-02, -5.9158e-03, -8.2899e-02,  1.8405e-02, -5.2977e-02,\n",
              "                        1.5472e-02,  2.5501e-03,  3.3595e-02,  6.5879e-02,  2.8866e-02,\n",
              "                        7.1254e-02,  4.3038e-02,  4.6825e-02, -6.4748e-02,  1.1567e-02,\n",
              "                       -3.7077e-02, -4.4515e-02,  4.4943e-02, -4.7061e-03, -1.1236e-01,\n",
              "                       -2.6365e-02,  6.7361e-03, -3.6260e-02, -2.5291e-02, -5.6529e-02,\n",
              "                       -1.0801e-01, -1.4322e-01, -7.8510e-02, -7.2656e-02,  3.7520e-02,\n",
              "                       -7.7128e-02, -8.3857e-02,  3.6517e-02, -7.0455e-02,  2.1513e-02,\n",
              "                        5.3544e-02,  3.8863e-02, -1.7519e-02, -1.0896e-01, -8.0313e-03,\n",
              "                       -3.6231e-02,  2.4121e-02, -2.6319e-02, -1.0116e-01, -8.1044e-02,\n",
              "                       -2.7931e-02,  4.4618e-02,  3.5626e-03],\n",
              "                      [-8.4781e-02,  6.4850e-02, -7.5671e-02,  2.3699e-02, -7.4625e-02,\n",
              "                       -3.5001e-02, -5.1452e-02,  6.0646e-02, -1.9823e-02, -6.0062e-02,\n",
              "                       -3.7701e-02, -9.8015e-02, -6.9274e-02, -7.0750e-02, -6.8950e-02,\n",
              "                       -1.8349e-02, -2.3890e-02,  3.2390e-02, -8.7428e-02,  7.5830e-02,\n",
              "                       -1.8955e-02,  2.7935e-02, -7.0111e-02, -8.6614e-02,  1.7065e-02,\n",
              "                       -4.2062e-02,  2.8664e-02, -8.2790e-02, -1.1087e-01, -5.8181e-02,\n",
              "                        1.5418e-02,  3.6185e-02,  5.2222e-02,  5.1282e-02, -8.3839e-02,\n",
              "                        6.1974e-02,  7.2776e-02, -8.3220e-02,  5.3873e-02,  4.9240e-02,\n",
              "                       -7.3891e-02,  6.4220e-02, -8.5215e-02,  6.0297e-02, -4.9073e-02,\n",
              "                       -1.1854e-01,  3.8445e-02,  7.6370e-02, -2.5002e-02, -2.9174e-02,\n",
              "                        7.9627e-02,  4.1947e-02,  1.8855e-02,  3.0064e-02,  4.2937e-02,\n",
              "                       -9.2537e-02, -3.9636e-02, -2.3560e-02, -4.3479e-02, -9.9900e-02,\n",
              "                       -1.0869e-01, -2.8227e-02, -5.5721e-02, -8.9429e-02,  4.7259e-03,\n",
              "                       -5.3316e-02,  2.1458e-02,  4.6369e-03, -4.1936e-02, -4.2308e-02,\n",
              "                        6.7180e-02, -5.3514e-02, -1.4729e-02,  2.8814e-02, -2.5988e-02,\n",
              "                       -5.2060e-02, -4.9528e-02, -9.9342e-02, -1.3918e-01,  7.7964e-02,\n",
              "                       -4.1404e-03, -2.5913e-02,  8.2147e-02, -2.7995e-02,  2.7891e-02,\n",
              "                       -4.7272e-02, -6.5152e-02,  1.1260e-02,  7.5280e-02, -7.5958e-02,\n",
              "                        6.0807e-02, -4.5569e-02,  6.2109e-02, -2.2182e-02, -8.4313e-02,\n",
              "                       -1.1115e-01,  7.7557e-02, -1.4710e-04,  6.8482e-02, -1.2470e-01,\n",
              "                        2.0777e-02,  4.8324e-02, -4.8476e-03, -6.7254e-02, -2.1238e-02,\n",
              "                        2.0178e-02, -1.1244e-01, -3.1672e-02,  7.6855e-02,  7.7024e-02,\n",
              "                        7.0009e-02, -9.0012e-02, -3.0965e-02, -1.7887e-02,  7.6746e-02,\n",
              "                       -8.1560e-02,  2.8723e-02,  7.3471e-02, -1.0315e-01,  4.4270e-02,\n",
              "                        6.9992e-02,  7.5639e-02,  8.1570e-02,  6.3030e-04,  2.1333e-03,\n",
              "                       -7.4683e-02, -4.7977e-03,  3.2861e-03]])),\n",
              "             ('fc_policy.bias',\n",
              "              tensor([-0.0590, -0.0297, -0.0815,  0.0560, -0.0392, -0.0713,  0.0287])),\n",
              "             ('fc_value.weight',\n",
              "              tensor([[ 4.1015e-02, -1.7014e-02,  3.4151e-02, -4.9785e-03, -2.2289e-02,\n",
              "                       -4.9842e-02, -7.8055e-02, -8.1363e-02, -6.2328e-02, -7.9601e-02,\n",
              "                        3.5168e-02, -1.9682e-02,  5.6518e-03, -1.4427e-02, -6.8237e-02,\n",
              "                        5.2766e-02,  4.3978e-02,  6.1954e-02, -6.5220e-02, -7.3642e-02,\n",
              "                       -1.6355e-02,  6.8921e-02,  1.4477e-03, -1.1391e-02,  4.9413e-02,\n",
              "                       -8.0640e-02,  9.4917e-03, -5.3408e-02, -5.4883e-02, -4.4757e-02,\n",
              "                       -4.7439e-02,  1.9232e-04, -4.3237e-02, -5.5200e-02, -7.2728e-03,\n",
              "                       -7.8725e-02, -6.5765e-02, -5.3143e-02,  5.1838e-02, -7.5781e-02,\n",
              "                        4.5467e-02, -8.8347e-03,  6.2058e-02, -7.4298e-02, -6.4204e-02,\n",
              "                        4.0426e-02, -5.4544e-03, -5.1555e-02,  1.7058e-02, -1.0312e-02,\n",
              "                       -7.5388e-02, -3.9862e-02,  1.9824e-02, -3.9396e-02,  4.8096e-03,\n",
              "                       -2.0122e-02,  4.1968e-02,  5.8679e-02, -8.6704e-03, -1.8514e-02,\n",
              "                       -2.8242e-02, -1.9937e-02,  1.2394e-02,  4.3794e-02,  4.2704e-02,\n",
              "                        7.4515e-02, -1.9131e-02, -7.8222e-02,  1.2562e-02,  2.1659e-02,\n",
              "                       -3.0418e-02,  3.4571e-02, -3.7174e-02,  5.4524e-02, -7.2475e-03,\n",
              "                       -1.3869e-02,  5.0362e-03, -3.7197e-02,  9.0137e-04, -1.6830e-02,\n",
              "                       -2.2178e-02, -6.2103e-02, -2.8230e-02, -6.1406e-02, -1.7142e-02,\n",
              "                       -8.1422e-02,  3.3810e-03,  6.8455e-02, -5.6626e-02,  4.6994e-02,\n",
              "                        6.9439e-02,  1.8209e-02,  3.8586e-02,  5.1005e-02, -4.2616e-02,\n",
              "                       -5.2014e-02, -7.8598e-02,  2.2147e-02,  5.0646e-02, -1.4398e-02,\n",
              "                       -2.9579e-02,  1.5321e-02, -1.5920e-02, -5.5192e-02,  5.3406e-03,\n",
              "                       -9.2183e-03,  6.4969e-02, -1.3053e-02, -2.4399e-02,  1.1828e-02,\n",
              "                       -4.9566e-02, -6.2050e-06, -4.5884e-02, -4.4530e-03,  2.8607e-02,\n",
              "                        1.2614e-02,  4.6244e-02,  6.3235e-02,  4.4527e-02, -8.0361e-02,\n",
              "                       -3.5839e-02,  2.7760e-02, -5.1086e-02, -3.8362e-02, -2.2830e-02,\n",
              "                       -5.1661e-02, -1.2307e-02,  7.3083e-03]])),\n",
              "             ('fc_value.bias', tensor([0.0792]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "with open('weights.json', 'w') as file:\n",
        "    json.dump(model_state_dict, file, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JVkFNzZcP9lS",
        "outputId": "5bab1ea2-4530-48e8-cae6-502586a64096"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type Tensor is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4361c39b83a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type Tensor is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_agent(observation, configuration):\n",
        "    import numpy as np\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    board = np.array(observation.board).reshape(6, 7)\n",
        "\n",
        "    model = ConnectXNet()\n",
        "\n",
        "    model_state_dict =\n",
        "    model.load_state_dict(torch.load(\"alphazero_connectx.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    mcts = MCTS(model)\n",
        "    action = mcts.search(board, num_simulations=50)\n",
        "    return int(action)"
      ],
      "metadata": {
        "id": "zHMQVZuELCQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjyzWTA9LGLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}